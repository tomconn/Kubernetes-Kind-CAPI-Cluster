# main.tf

provider "kind" {
  # Provider block still needed, even if not creating cluster resource directly
}

# Define the kind configuration in a separate file
# Ensure kind-config.yaml exists with the 1-cp, 2-worker setup

# Use null_resource to manage the kind cluster lifecycle via CLI commands
resource "null_resource" "kind_cluster_manager" {
  # Triggers define when the cluster should be recreated
  triggers = {
    # IMPORTANT: Keep the var reference here for trigger evaluation on plan/apply
    cluster_name = var.cluster_name
    node_image   = var.kind_node_image
    config_sha1  = filesha1("${path.module}/kind-config.yaml")
  }

  # Create the cluster using local-exec
  provisioner "local-exec" {
    when    = create
    # Reference the trigger value for the commands during creation
    command = <<EOT
      echo "Creating Kind cluster '${self.triggers.cluster_name}' with config '${path.module}/kind-config.yaml'..."
      kind create cluster \
        --name "${self.triggers.cluster_name}" \
        --image "${self.triggers.node_image}" \
        --config "${path.module}/kind-config.yaml" \
        --wait 5m && \
      echo "Kind cluster created. Saving kubeconfig..." && \
      kind get kubeconfig --name "${self.triggers.cluster_name}" > "${path.module}/kubeconfig-${self.triggers.cluster_name}.yaml" && \
      echo "Kubeconfig saved to ${path.module}/kubeconfig-${self.triggers.cluster_name}.yaml"
    EOT
    interpreter = ["bash", "-c"]
  }

  # Destroy the cluster using local-exec
  provisioner "local-exec" {
    when    = destroy
    # Reference the cluster name via the 'self.triggers' map, which is allowed
    command     = "echo 'Deleting Kind cluster ${self.triggers.cluster_name}...' && kind delete cluster --name ${self.triggers.cluster_name}"
    interpreter = ["bash", "-c"]
    # Optional: Make destroy more robust if kind delete fails
    # on_failure = continue
  }
}

# Use local-exec provisioner within a null_resource to run clusterctl init
resource "null_resource" "capi_init" {
  depends_on = [null_resource.kind_cluster_manager]

  triggers = {
    # Depend on the manager resource's state (ID changes if recreated)
    cluster_manager_id = null_resource.kind_cluster_manager.id
    providers_list     = join(",", var.capi_providers)
    # Pass the cluster name from the manager's triggers for use below
    cluster_name       = null_resource.kind_cluster_manager.triggers.cluster_name
  }

  provisioner "local-exec" {
    # Use the kubeconfig file generated by the kind_cluster_manager provisioner
    # Reference the cluster_name from this resource's own triggers
    command = <<EOT
      export KUBECONFIG="${path.module}/kubeconfig-${self.triggers.cluster_name}.yaml" && \
      echo "Waiting for nodes in cluster '${self.triggers.cluster_name}' to be ready..." && \
      kubectl wait --for=condition=Ready node --all --timeout=3m && \
      echo "Nodes ready. Waiting a bit more for API stability..." && \
      sleep 10 && \
      echo "Initializing Cluster API providers: ${join(",", var.capi_providers)}..." && \
      clusterctl init --infrastructure ${join(",", formatlist("%s", var.capi_providers))} && \
      echo "Cluster API initialization command executed."
    EOT
    interpreter = ["bash", "-c"]
  }
}
